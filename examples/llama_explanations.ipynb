{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38172a9e",
   "metadata": {},
   "source": [
    "# üßæ Explanation Generation for DSM-5 Classification using LLaMA + Instructor API\n",
    "\n",
    "This notebook explores how to generate **rationale-based explanations** for multi-label classification decisions over the `ReDSM5` dataset. Using the `instructor` library and a local LLaMA model via Ollama, we produce structured outputs that include both:\n",
    "- a **natural language explanation**, and\n",
    "- a **chain-of-thought reasoning trace**.\n",
    "\n",
    "The workflow includes:\n",
    "- Loading the data and preparing test inputs,\n",
    "- Defining structured output types using `pydantic`,\n",
    "- Asynchronous, rate-limited inference using Ollama,\n",
    "- Saving outputs for evaluation using metrics such as ROUGE and BERTScore.\n",
    "\n",
    "This pipeline is part of a larger effort to build interpretable and clinically aligned NLP systems for mental health research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7cd36d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Environment Setup and Imports\n",
    "\n",
    "We install the `instructor` library, which wraps OpenAI-compatible clients to enforce structured outputs based on `pydantic` schemas.\n",
    "\n",
    "We also import standard libraries for:\n",
    "- I/O (`json`, `requests`, `pandas`, etc.),\n",
    "- Data splitting and preprocessing,\n",
    "- Async inference with semaphores to avoid overload,\n",
    "- Schema validation with `pydantic`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90636f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: instructor in /usr/local/lib/python3.11/site-packages (1.8.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.11/site-packages (from instructor) (3.11.18)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.11/site-packages (from instructor) (0.16)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.11/site-packages (from instructor) (3.1.6)\n",
      "Requirement already satisfied: jiter<0.9,>=0.6.1 in /usr/local/lib/python3.11/site-packages (from instructor) (0.8.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.70.0 in /usr/local/lib/python3.11/site-packages (from instructor) (1.83.0)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/site-packages (from instructor) (2.33.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/site-packages (from instructor) (2.11.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/site-packages (from instructor) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.11/site-packages (from instructor) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/site-packages (from instructor) (9.1.2)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.11/site-packages (from instructor) (0.15.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.4->instructor) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/site-packages (from openai<2.0.0,>=1.70.0->instructor) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai<2.0.0,>=1.70.0->instructor) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from openai<2.0.0,>=1.70.0->instructor) (0.28.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from openai<2.0.0,>=1.70.0->instructor) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/site-packages (from openai<2.0.0,>=1.70.0->instructor) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/site-packages (from openai<2.0.0,>=1.70.0->instructor) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.8.0->instructor) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.8.0->instructor) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->instructor) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->instructor) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->instructor) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->instructor) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.19.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0.0,>=0.9.0->instructor) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.70.0->instructor) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.70.0->instructor) (0.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/site-packages (from rouge-score) (2.2.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/site-packages (from nltk->rouge-score) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from nltk->rouge-score) (4.67.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: bert-score in /usr/local/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/site-packages (from bert-score) (2.7.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/site-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/site-packages (from bert-score) (4.51.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/site-packages (from bert-score) (3.10.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/site-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.3.0->torch>=1.0.0->bert-score) (65.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->bert-score) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/site-packages (from matplotlib->bert-score) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->bert-score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->bert-score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->bert-score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->bert-score) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from typing import List, Optional, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d9dad",
   "metadata": {},
   "source": [
    "## üîå Connecting to Ollama (LLaMA) via Instructor\n",
    "\n",
    "We connect to a locally running Ollama instance serving a LLaMA model using OpenAI-compatible endpoints. The `instructor.from_openai()` wrapper enforces output constraints defined by a `pydantic` schema.\n",
    "\n",
    "A semaphore is used to limit concurrency and avoid overwhelming the local model server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a94bb40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "client = instructor.from_openai(client, mode=instructor.Mode.JSON)\n",
    "\n",
    "sem = asyncio.Semaphore(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce10180",
   "metadata": {},
   "source": [
    "## üìä Loading and Preparing ReDSM5 Data\n",
    "\n",
    "We load the `ReDSM5` dataset and extract the input `text` and target `explanation` fields. Then we split the data into training and test sets using an 80/20 ratio.\n",
    "\n",
    "The test portion is saved to disk in `.jsonl` format for reproducibility and batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "01c6708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/redsm5.csv\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "explanations = data[\"explanation\"].tolist()\n",
    "texts = data[\"text\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, explanations, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "with open(\"data/redsm5_test_explanations_inputs.jsonl\", \"w\") as f:\n",
    "    for text, explanation in zip(X_test, y_test):\n",
    "        f.write(json.dumps({\"text\": text, \"explanation\": explanation}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c8a8a",
   "metadata": {},
   "source": [
    "## üìê Defining Structured Explanation Schema\n",
    "\n",
    "We use `pydantic` to define the structure of the model‚Äôs output:\n",
    "- `explanation`: a plain-language rationale tied to specific text spans,\n",
    "- `chain_of_thought`: a free-form reasoning trace of how the model inferred symptom presence or absence.\n",
    "\n",
    "This structure ensures explanations are well-formed and interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c4702266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explanation(BaseModel):\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=\"Plain-language rationale that cites or paraphrases phrases from the text to justify why each symptom was tagged present or absent.\",\n",
    "    )\n",
    "\n",
    "    chain_of_thought: str = Field(\n",
    "        ..., description=\"The chain of thought that led to the explanation.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39031008",
   "metadata": {},
   "source": [
    "## üß† Prompted Explanation Generation with Chain-of-Thought\n",
    "\n",
    "We define an asynchronous function `explain()` that sends a structured prompt to the local Ollama instance running `gemma3:27b-it-qat`.\n",
    "\n",
    "- The prompt uses few-shot examples pulled from the training set (`X_train`, `y_train`) to guide the model.\n",
    "- A `system` role sets the context: the model acts as a mental-health assistant, providing DSM-5-based rationales.\n",
    "- The output is parsed and validated against the `Explanation` schema, which ensures presence of both a human-readable rationale and a chain of thought.\n",
    "\n",
    "This function is wrapped in a semaphore to manage concurrency and retry logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "98cf9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def explain(data: str) -> Explanation:\n",
    "    def _fewshot_messages(k=10):\n",
    "        msgs = []\n",
    "        for x, y in zip(X_train[:k], y_train[:k]):\n",
    "            msgs.append({\"role\": \"user\", \"content\": x})\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": y})\n",
    "        return msgs\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            result = await client.chat.completions.create(\n",
    "                model=\"gemma3:27b-it-qat\",\n",
    "                response_model=Explanation,\n",
    "                max_retries=5,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"You are an expert mental-health assistant. \"\n",
    "                            \"Your job is to read a first-person text (e.g. a social-media post) and explain which DSM-5 A-criteria symptoms for Major Depressive Disorder are directly present or explicitly absent in that text.\"\n",
    "                        ),\n",
    "                    },\n",
    "                    *(_fewshot_messages(k=50)),\n",
    "                    {\"role\": \"user\", \"content\": data},\n",
    "                ],\n",
    "            )\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error during API call: {e}\")\n",
    "            return Explanation(\n",
    "                explanation=\"Error during explanation.\",\n",
    "                chain_of_thought=\"Error during explanation.\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1564e",
   "metadata": {},
   "source": [
    "## üì¨ Batch Processing and Output Writing\n",
    "\n",
    "We define `process_inputs()` to loop through a list of test inputs, call the `explain()` function on each, and store the results.\n",
    "\n",
    "Each output contains:\n",
    "- the original text,\n",
    "- the ground-truth explanation (`explanation_true`),\n",
    "- the predicted explanation and reasoning trace.\n",
    "\n",
    "All results are saved in JSON Lines format to support later evaluation and inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "802268e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_inputs(inputs: List[dict], output_path: Optional[str] = None) -> None:\n",
    "    for input_data in inputs:\n",
    "        prediction = await explain(input_data[\"text\"])\n",
    "\n",
    "        resp = {\n",
    "            \"text\": input_data[\"text\"],\n",
    "            \"explanation_true\": input_data[\"explanation\"],\n",
    "            \"explanation_predicted\": prediction.explanation,\n",
    "            \"chain_of_thought\": prediction.chain_of_thought,\n",
    "        }\n",
    "\n",
    "        if output_path:\n",
    "            with open(output_path, \"a\") as f:\n",
    "                json.dump(resp, f)\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5324d78a",
   "metadata": {},
   "source": [
    "## üß™ Inference on the Test Set\n",
    "\n",
    "We read the previously saved test inputs (`redsm5_test_explanations_inputs.jsonl`) and run the asynchronous generation pipeline.\n",
    "\n",
    "Predictions are written to `redsm5_test_explanations_outputs.jsonl`. Errors or schema mismatches are printed to the console for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d933952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during API call: 1 validation error for Explanation\n",
      "chain_of_thought\n",
      "  Field required [type=missing, input_value={'explanation': \"This is ...nation': 'This is ...}\"}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "Error during API call: 1 validation error for Explanation\n",
      "chain_of_thought\n",
      "  Field required [type=missing, input_value={'explanation': \"This nar...'mental health crisis']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "Error during API call: 1 validation error for Explanation\n",
      "chain_of_thought\n",
      "  Field required [type=missing, input_value={'explanation': \"The text... deception', 'despair']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "Error during API call: 1 validation error for Explanation\n",
      "  Invalid JSON: EOF while parsing an object at line 22 column 2 [type=json_invalid, input_value='{\\n  \"explanation\": \"Thi... \\n\\n  \\n\\n  \\n  \\n\\n  ', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/redsm5_test_explanations_inputs.jsonl\", \"r\") as f:\n",
    "    inputs = [json.loads(line) for line in f]\n",
    "\n",
    "nest_asyncio.apply()\n",
    "await process_inputs(inputs, output_path=\"data/redsm5_test_explanations_outputs.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac96456",
   "metadata": {},
   "source": [
    "## üîó Embedding-Based Evaluation\n",
    "\n",
    "We define `get_ollama_embedding()` to generate sentence embeddings for a given explanation using Ollama‚Äôs embedding endpoint (e.g., `nomic-embed-text`).\n",
    "\n",
    "These embeddings will be used to compute cosine similarity between the ground-truth and predicted explanations, as a meaning-based evaluation metric (complementary to n-gram methods like ROUGE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3b5e19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_embedding(text, model=\"nomic-embed-text\"):\n",
    "    url = \"http://localhost:11434/api/embed\"\n",
    "    payload = {\"model\": model, \"input\": text}\n",
    "    resp = requests.post(url, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    embedding = data.get(\"embeddings\", [])\n",
    "    if not embedding:\n",
    "        print(f\"[WARN] Empty embedding for text: '{text[:60]}...'\")\n",
    "        return np.zeros(768, dtype=np.float32)\n",
    "    if isinstance(embedding[0], list):\n",
    "        embedding = embedding[0]\n",
    "    return np.array(embedding, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7aaa6",
   "metadata": {},
   "source": [
    "## üìè Semantic Similarity with Cosine Distance\n",
    "\n",
    "We implement `compute_embedding_similarity()` to:\n",
    "- Read predicted and true explanations from the JSONL file,\n",
    "- Embed both using Ollama's embedding API,\n",
    "- Compute cosine similarity between each pair,\n",
    "- Report the average similarity as a proxy for how semantically aligned the predictions are with the human-written explanations.\n",
    "\n",
    "This offers a flexible evaluation signal that tolerates paraphrasing and rewording.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "56950461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_similarity(jsonl_path, model=\"nomic-embed-text\"):\n",
    "    refs_pred, hyps_pred = [], []\n",
    "    entries = []\n",
    "\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            true = obj[\"explanation_true\"]\n",
    "            pred = obj[\"explanation_predicted\"]\n",
    "            entries.append((true, pred))\n",
    "            refs_pred.append(true)\n",
    "            hyps_pred.append(pred)\n",
    "\n",
    "    gptscore_pred = []\n",
    "    for true, pred in entries:\n",
    "        emb_true = get_ollama_embedding(true, model)\n",
    "        emb_pred = get_ollama_embedding(pred, model)\n",
    "\n",
    "        norm_true = np.linalg.norm(emb_true)\n",
    "        norm_pred = np.linalg.norm(emb_pred)\n",
    "\n",
    "        if norm_true == 0 or norm_pred == 0:\n",
    "            cos_pred = 0.0\n",
    "        else:\n",
    "            cos_pred = np.dot(emb_true, emb_pred) / (norm_true * norm_pred)\n",
    "            cos_pred = float(np.clip(cos_pred, -1.0, 1.0))\n",
    "\n",
    "        gptscore_pred.append(cos_pred)\n",
    "\n",
    "    def avg(scores):\n",
    "        return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "    print(f\"Avg. embedding similarity: {avg(gptscore_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048b960",
   "metadata": {},
   "source": [
    "## üìà Similarity Results\n",
    "\n",
    "We call `compute_embedding_similarity()` to compute the average cosine similarity between ground-truth and predicted explanations using `nomic-embed-text` embeddings.\n",
    "\n",
    "This provides a semantics-aware score that reflects how close the generated rationales are to the expert annotations, even if they differ in surface form.\n",
    "\n",
    "> ‚úÖ Result: A mean similarity of ~0.78 suggests the model generates mostly on-topic and clinically relevant explanations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0defea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. embedding similarity: 0.7846\n"
     ]
    }
   ],
   "source": [
    "compute_embedding_similarity(\n",
    "    jsonl_path=\"data/redsm5_test_explanations_outputs.jsonl\",\n",
    "    model=\"nomic-embed-text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b673d98",
   "metadata": {},
   "source": [
    "## üß™ LLM-Based Explanation Judging\n",
    "\n",
    "To complement embedding-based metrics, we define a `JudgeScores` schema that includes:\n",
    "- `accuracy`: agreement with clinical facts,\n",
    "- `coverage`: how completely the explanation addresses relevant symptoms,\n",
    "- `clarity`: readability and fluency.\n",
    "\n",
    "These are rated by a second LLM (`deepseek-r1 8B`) acting as an expert evaluator using a few-shot rubric. The judge returns a numeric score (1‚Äì5) for each aspect and a brief justification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c20a8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeScores(BaseModel):\n",
    "    accuracy: int\n",
    "    coverage: int\n",
    "    clarity: int\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e7e10",
   "metadata": {},
   "source": [
    "## ü§ñ Explanation Rating with Chain-of-Thought Rubric\n",
    "\n",
    "The `llm_judge()` function prompts the LLM to compare a candidate explanation against the expert reference using natural language instructions and structured output validation.\n",
    "\n",
    "A weighted average score is computed using:\n",
    "- 40% for accuracy,\n",
    "- 30% for coverage,\n",
    "- 30% for clarity.\n",
    "\n",
    "This simulates human-style evaluation while ensuring standardization across examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3b9bd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def llm_judge(\n",
    "    reference: str, candidate: str, model: str\n",
    ") -> Tuple[float, JudgeScores]:\n",
    "    WEIGHTS = {\"accuracy\": 0.40, \"coverage\": 0.30, \"clarity\": 0.30}\n",
    "\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert evaluator of short clinical explanations.\\n\"\n",
    "            \"Rate the CANDIDATE against the REFERENCE with this rubric:\\n\"\n",
    "            \"1. Clinical Accuracy (1-5): Does the candidate broadly match the clinical statements? \"\n",
    "            \"If there are no significant contradictions and the main points align, you may rate 4 or 5.\\n\"\n",
    "            \"2. Coverage (1-5): If the candidate mentions most of the important symptoms or absences described in the reference, and does not invent major content, you may give a high score. \"\n",
    "            \"Minor omissions are acceptable if the main message remains the same.\\n\"\n",
    "            \"3. Clarity & Conciseness (1-5): Is the candidate easy to read and generally clear?\\n\\n\"\n",
    "            \"If the candidate explanation broadly covers the same clinical content or intent as the reference, you may give a high score, even if the phrasing or minor details differ.\\n\"\n",
    "            \"Return ONLY a JSON object with keys accuracy, coverage, clarity and explanation (‚â§2 lines).\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"REFERENCE:\\n<<<{reference}>>>\\n\\n\" f\"CANDIDATE:\\n<<<{candidate}>>>\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    async with sem:\n",
    "        try:\n",
    "            result: JudgeScores = await client.chat.completions.create(\n",
    "                model=model,\n",
    "                response_model=JudgeScores,\n",
    "                max_retries=5,\n",
    "                messages=[system_msg, user_msg],\n",
    "            )\n",
    "        except Exception as e:\n",
    "            result = JudgeScores(\n",
    "                accuracy=3, coverage=3, clarity=3, explanation=\"Error during judging.\"\n",
    "            )\n",
    "\n",
    "    overall = sum(WEIGHTS[k] * (getattr(result, k) - 1) / 4 for k in WEIGHTS) * 100\n",
    "    return overall, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e838f6",
   "metadata": {},
   "source": [
    "## üì§ Batch Evaluation of LLM-Generated Explanations\n",
    "\n",
    "We define `evaluate_jsonl_llm_judge()` to:\n",
    "- Iterate over all test examples,\n",
    "- Send each reference/prediction pair to the judge LLM,\n",
    "- Collect and average the evaluation scores.\n",
    "\n",
    "This provides a final model-level score based on human-aligned qualitative criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0728b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_jsonl_llm_judge(path: str, model: str) -> None:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    async def judge(ref: str, hyp: str):\n",
    "        score, _ = await llm_judge(ref, hyp, model)\n",
    "        return score\n",
    "\n",
    "    tasks_pred = []\n",
    "    for obj in data:\n",
    "        ref = obj[\"explanation_true\"]\n",
    "        pred = obj[\"explanation_predicted\"]\n",
    "        tasks_pred.append(judge(ref, pred))\n",
    "\n",
    "    pred_scores = await asyncio.gather(*tasks_pred)\n",
    "\n",
    "    print(f\"Avg. llm judge score: {np.mean(pred_scores):6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b88951",
   "metadata": {},
   "source": [
    "## üßæ Final Results: LLM Judge Score\n",
    "\n",
    "We run the evaluation on all test examples and print the average judge score. A result around 61.5 (on a 0‚Äì100 scale) suggests that most generated explanations are accurate, reasonably complete, and clearly written, though there is still room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fd9a25ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. llm judge score:  61.50\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(\n",
    "    evaluate_jsonl_llm_judge(\n",
    "        \"data/redsm5_test_explanations_outputs.jsonl\", model=\"deepseek-r1:8b\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
